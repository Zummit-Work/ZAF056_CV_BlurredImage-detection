{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "842dfbe8",
   "metadata": {},
   "source": [
    "# (2) CV WINTER 2023 - Blurred Image Detection via Supervised Learning\n",
    "\n",
    "### Project Description\n",
    "There is a dataset of images, par of which is blurred. The task is to develop a machine learning algorithm to detect whether the image is blurred or sharp on the unknown dataset.\n",
    "\n",
    "### Data Description\n",
    "The dataset consists of images, and some of them are blurred. The images are blurred using augmentation.\n",
    "\n",
    "Data files:\n",
    "- ```train``` - a folder for training\n",
    "- ```test``` - a folder with images, for which we make predicitons\n",
    "- ```train.csv``` - labels (answers) to the train sample: **if 1, the image is blurred.**\n",
    "\n",
    "### Papers:\n",
    "- https://jiaya.me/all_final_papers/blur_detect_cvpr08.pdf\n",
    "- https://fled.github.io/paper/blur.pdf\n",
    "- http://graphics.im.ntu.edu.tw/docs/mmm08.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "c3b551f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.11.0\n",
      "4.7.0\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from sklearn import svm\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Input, Conv2D, Dense, Flatten, Dropout, Activation\n",
    "from tensorflow.keras.layers import GlobalMaxPooling2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "# show the version of tf\n",
    "print(tf.__version__)\n",
    "print(cv2.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddb5784",
   "metadata": {},
   "source": [
    "## 1 Data Load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cea76f",
   "metadata": {},
   "source": [
    "### 1.1 Loading images and labels from given datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "c80ba948",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>blur</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kagouracdzwrjjxzzedi.jpg</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ahnamimqdfqoqdnozabc.jpg</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gwhdadvghuzinmzhzssx.jpg</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>onqwabwwckubrydgbzly.jpg</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ewpqdruddbokqyzzupcw.jpg</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   filename  blur\n",
       "0  kagouracdzwrjjxzzedi.jpg   0.0\n",
       "1  ahnamimqdfqoqdnozabc.jpg   0.0\n",
       "2  gwhdadvghuzinmzhzssx.jpg   0.0\n",
       "3  onqwabwwckubrydgbzly.jpg   0.0\n",
       "4  ewpqdruddbokqyzzupcw.jpg   1.0"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = pd.read_csv('/Users/yuliabezginova/PycharmProjects/00_files-for_NLP/shift-cv-winter-2023/train.csv')\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "3cbacbc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2579 images belonging to 2 classes.\n",
      "Found 859 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    validation_split=0.25,\n",
    "    rescale=1./255,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    rotation_range=90,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2\n",
    ")\n",
    "\n",
    "validation_datagen = ImageDataGenerator(\n",
    "    validation_split=0.25,\n",
    "    rescale=1./255)\n",
    "\n",
    "# Чтобы загрузчик извлёк данные из папки, вызовем функцию \n",
    "# flow_from_directory() (англ. «поток из директории»):\n",
    "\n",
    "train_datagen_flow = train_datagen.flow_from_directory(\n",
    "    '/Users/yuliabezginova/PycharmProjects/00_files-for_NLP/shift-cv-winter-2023/',\n",
    "    target_size=(150, 150),\n",
    "    batch_size=16,\n",
    "    class_mode='sparse',\n",
    "    subset='training',\n",
    "    seed=12345)\n",
    "\n",
    "val_datagen_flow = validation_datagen.flow_from_directory(\n",
    "    '/Users/yuliabezginova/PycharmProjects/00_files-for_NLP/shift-cv-winter-2023/',\n",
    "    target_size=(150, 150),\n",
    "    batch_size=16,\n",
    "    class_mode='sparse',\n",
    "    subset='validation',\n",
    "    seed=12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7830ab",
   "metadata": {},
   "source": [
    "### 1.2 Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2417265f",
   "metadata": {},
   "source": [
    "#### 1.2.1 Understanding the variables type, labels structure, number of target classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06002bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f843581c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels['blur'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a04be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdd69d5",
   "metadata": {},
   "source": [
    "#### 1.2.2 Descriptive statistics of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1326fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6136affd",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels['blur'].hist(bins=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf14df3",
   "metadata": {},
   "source": [
    "#### 1.2.3 Checking the target value for missing and duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e564ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c63ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32566614",
   "metadata": {},
   "source": [
    "#### 1.2.4  Checking the target value for disbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dae214e",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df = pd.DataFrame(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8822531",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df['blur'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099b911e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_blur = labels[labels[\"blur\"]==1]\n",
    "train_sharp = labels[labels[\"blur\"]==0]\n",
    "\n",
    "print(\"Blur images:\", len(train_blur))\n",
    "print(\"Sharp images:\", len(train_sharp))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c458330f",
   "metadata": {},
   "source": [
    "### ***Conclusion:*** Type of variable in the target values is ```float64``` with two unique values - 0, 1. This is fine for solving classification problem, but the target should be converted into ```string``` type. Number of observations if 2664. The target dataset is balanced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d184a07a",
   "metadata": {},
   "source": [
    "## 2 Constructing CNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "895745af",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/yuliabezginova/PycharmProjects/00_files-for_NLP/shift-cv-winter-2023/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "81c779d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_train(path):\n",
    "    labels = pd.read_csv(path + 'train.csv')\n",
    "    train_datagen = ImageDataGenerator(validation_split=0.25, horizontal_flip=True, rescale=1./255)\n",
    "    train_gen_flow = train_datagen.flow_from_dataframe(\n",
    "        dataframe=labels,\n",
    "        directory=path + 'train/train/',\n",
    "        x_col='filename',\n",
    "        y_col='blur',\n",
    "        target_size=(96, 96),\n",
    "        batch_size=32,\n",
    "        class_mode='raw',\n",
    "        subset='training',\n",
    "        seed=5)\n",
    "\n",
    "    return train_gen_flow\n",
    "\n",
    "\n",
    "def load_test(path):\n",
    "    labels = pd.read_csv(path + 'train.csv')\n",
    "    test_datagen = ImageDataGenerator(validation_split=0.25, rescale=1./255)\n",
    "    test_gen_flow = test_datagen.flow_from_dataframe(\n",
    "        dataframe=labels,\n",
    "        directory=path + 'train/train/',\n",
    "        x_col='filename',\n",
    "        y_col='blur',\n",
    "        target_size=(96, 96),\n",
    "        batch_size=32,\n",
    "        class_mode='raw',\n",
    "        subset='validation',\n",
    "        seed=5)\n",
    "\n",
    "    return test_gen_flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "a50678c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1998 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "train = load_train(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "c6bd2284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 666 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "valid = load_test(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "61c9f507",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet import ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "1a255dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_ResNet50(input_shape):    \n",
    "    backbone = ResNet50(input_shape=input_shape,\n",
    "                    weights='imagenet', \n",
    "                    include_top=False)\n",
    "    \n",
    "#     замораживаем ResNet50 без верхушки\n",
    "#     backbone.trainable = False\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(backbone)\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "    model.add(Dropout(0.25))\n",
    "#     model.add(BatchNormalization())\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1, activation='softmax')) \n",
    "\n",
    "    optimizer = Adam(learning_rate=0.0001)\n",
    "    model.compile(optimizer=optimizer, loss = \"binary_crossentropy\",\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "e7b88c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape):\n",
    "    model = Sequential()\n",
    "#     model.add(layers.Reshape((96, 96, 2), input_shape=input_shape))\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', input_shape=input_shape))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Conv2D(32, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "#     model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "#     model.add(BatchNormalization())\n",
    "    model.add(Flatten())    \n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "#     model.add(BatchNormalization())\n",
    "    model.add(Dense(1, activation='softmax'))\n",
    "    \n",
    "    optimizer = Adam(learning_rate=0.0001)\n",
    "    \n",
    "    model.compile(optimizer, loss = \"categorical_crossentropy\",\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "81661502",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_data, test_data, batch_size=None, epochs=30,\n",
    "                steps_per_epoch=None, validation_steps=None):\n",
    "\n",
    "    if steps_per_epoch is None:\n",
    "        steps_per_epoch = len(train_data)\n",
    "    if validation_steps is None:\n",
    "        validation_steps = len(test_data)\n",
    "\n",
    "    model.fit(train_data,\n",
    "              validation_data=test_data,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              steps_per_epoch=steps_per_epoch,\n",
    "              validation_steps=validation_steps,\n",
    "              verbose=2)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "5dc84d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (96, 96, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "9ecf47e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initiating the model\n",
    "model = create_model(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "80bd8b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_39\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_69 (Conv2D)          (None, 96, 96, 64)        1792      \n",
      "                                                                 \n",
      " activation_79 (Activation)  (None, 96, 96, 64)        0         \n",
      "                                                                 \n",
      " conv2d_70 (Conv2D)          (None, 94, 94, 32)        18464     \n",
      "                                                                 \n",
      " activation_80 (Activation)  (None, 94, 94, 32)        0         \n",
      "                                                                 \n",
      " max_pooling2d_32 (MaxPoolin  (None, 47, 47, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_45 (Dropout)        (None, 47, 47, 32)        0         \n",
      "                                                                 \n",
      " max_pooling2d_33 (MaxPoolin  (None, 23, 23, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_46 (Dropout)        (None, 23, 23, 32)        0         \n",
      "                                                                 \n",
      " flatten_15 (Flatten)        (None, 16928)             0         \n",
      "                                                                 \n",
      " dense_54 (Dense)            (None, 10)                169290    \n",
      "                                                                 \n",
      " activation_81 (Activation)  (None, 10)                0         \n",
      "                                                                 \n",
      " dropout_47 (Dropout)        (None, 10)                0         \n",
      "                                                                 \n",
      " dense_55 (Dense)            (None, 1)                 11        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 189,557\n",
      "Trainable params: 189,557\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "596d9397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiating the model\n",
    "model_ResNet50 = create_model_ResNet50(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "ee1b46bc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "63/63 - 316s - loss: 0.6404 - accuracy: 0.4850 - val_loss: 1.4745 - val_accuracy: 0.4925 - 316s/epoch - 5s/step\n",
      "Epoch 2/10\n",
      "63/63 - 271s - loss: 0.2949 - accuracy: 0.4850 - val_loss: 0.8885 - val_accuracy: 0.4925 - 271s/epoch - 4s/step\n",
      "Epoch 3/10\n",
      "63/63 - 273s - loss: 0.1924 - accuracy: 0.4850 - val_loss: 1.0560 - val_accuracy: 0.4925 - 273s/epoch - 4s/step\n",
      "Epoch 4/10\n",
      "63/63 - 298s - loss: 0.1337 - accuracy: 0.4850 - val_loss: 0.7147 - val_accuracy: 0.4925 - 298s/epoch - 5s/step\n",
      "Epoch 5/10\n",
      "63/63 - 295s - loss: 0.1337 - accuracy: 0.4850 - val_loss: 3.5372 - val_accuracy: 0.4925 - 295s/epoch - 5s/step\n",
      "Epoch 6/10\n",
      "63/63 - 334s - loss: 0.1000 - accuracy: 0.4850 - val_loss: 3.1512 - val_accuracy: 0.4925 - 334s/epoch - 5s/step\n",
      "Epoch 7/10\n",
      "63/63 - 410s - loss: 0.0860 - accuracy: 0.4850 - val_loss: 0.8635 - val_accuracy: 0.4925 - 410s/epoch - 7s/step\n",
      "Epoch 8/10\n",
      "63/63 - 380s - loss: 0.0840 - accuracy: 0.4850 - val_loss: 1.0129 - val_accuracy: 0.4925 - 380s/epoch - 6s/step\n",
      "Epoch 9/10\n",
      "63/63 - 393s - loss: 0.0840 - accuracy: 0.4850 - val_loss: 0.5992 - val_accuracy: 0.4925 - 393s/epoch - 6s/step\n",
      "Epoch 10/10\n",
      "63/63 - 359s - loss: 0.0724 - accuracy: 0.4850 - val_loss: 0.5353 - val_accuracy: 0.4925 - 359s/epoch - 6s/step\n"
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "ResNet50_model = train_model(model_ResNet50, \n",
    "                             train, \n",
    "                             valid, \n",
    "                             batch_size=32, \n",
    "                             epochs=10, \n",
    "                             steps_per_epoch=None, \n",
    "                             validation_steps=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "e727ae72",
   "metadata": {},
   "outputs": [],
   "source": [
    "STEP_SIZE_TRAIN=train.n//train.batch_size\n",
    "STEP_SIZE_VALID=valid.n//valid.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "c4db855a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "62/62 - 81s - loss: 0.0000e+00 - accuracy: 0.4842 - val_loss: 0.0000e+00 - val_accuracy: 0.4925 - 81s/epoch - 1s/step\n",
      "Epoch 2/30\n",
      "62/62 - 65s - loss: 0.0000e+00 - accuracy: 0.4852 - val_loss: 0.0000e+00 - val_accuracy: 0.4925 - 65s/epoch - 1s/step\n",
      "Epoch 3/30\n",
      "62/62 - 77s - loss: 0.0000e+00 - accuracy: 0.4842 - val_loss: 0.0000e+00 - val_accuracy: 0.4925 - 77s/epoch - 1s/step\n",
      "Epoch 4/30\n",
      "62/62 - 62s - loss: 0.0000e+00 - accuracy: 0.4868 - val_loss: 0.0000e+00 - val_accuracy: 0.4925 - 62s/epoch - 994ms/step\n",
      "Epoch 5/30\n",
      "62/62 - 57s - loss: 0.0000e+00 - accuracy: 0.4847 - val_loss: 0.0000e+00 - val_accuracy: 0.4925 - 57s/epoch - 926ms/step\n",
      "Epoch 6/30\n",
      "62/62 - 59s - loss: 0.0000e+00 - accuracy: 0.4832 - val_loss: 0.0000e+00 - val_accuracy: 0.4925 - 59s/epoch - 951ms/step\n",
      "Epoch 7/30\n",
      "62/62 - 59s - loss: 0.0000e+00 - accuracy: 0.4858 - val_loss: 0.0000e+00 - val_accuracy: 0.4925 - 59s/epoch - 947ms/step\n",
      "Epoch 8/30\n",
      "62/62 - 65s - loss: 0.0000e+00 - accuracy: 0.4847 - val_loss: 0.0000e+00 - val_accuracy: 0.4925 - 65s/epoch - 1s/step\n",
      "Epoch 9/30\n",
      "62/62 - 64s - loss: 0.0000e+00 - accuracy: 0.4832 - val_loss: 0.0000e+00 - val_accuracy: 0.4925 - 64s/epoch - 1s/step\n",
      "Epoch 10/30\n",
      "62/62 - 60s - loss: 0.0000e+00 - accuracy: 0.4847 - val_loss: 0.0000e+00 - val_accuracy: 0.4925 - 60s/epoch - 962ms/step\n",
      "Epoch 11/30\n",
      "62/62 - 61s - loss: 0.0000e+00 - accuracy: 0.4868 - val_loss: 0.0000e+00 - val_accuracy: 0.4925 - 61s/epoch - 987ms/step\n",
      "Epoch 12/30\n",
      "62/62 - 62s - loss: 0.0000e+00 - accuracy: 0.4832 - val_loss: 0.0000e+00 - val_accuracy: 0.4925 - 62s/epoch - 999ms/step\n",
      "Epoch 13/30\n",
      "62/62 - 63s - loss: 0.0000e+00 - accuracy: 0.4837 - val_loss: 0.0000e+00 - val_accuracy: 0.4925 - 63s/epoch - 1s/step\n",
      "Epoch 14/30\n",
      "62/62 - 74s - loss: 0.0000e+00 - accuracy: 0.4847 - val_loss: 0.0000e+00 - val_accuracy: 0.4925 - 74s/epoch - 1s/step\n",
      "Epoch 15/30\n",
      "62/62 - 62s - loss: 0.0000e+00 - accuracy: 0.4852 - val_loss: 0.0000e+00 - val_accuracy: 0.4925 - 62s/epoch - 995ms/step\n",
      "Epoch 16/30\n",
      "62/62 - 70s - loss: 0.0000e+00 - accuracy: 0.4858 - val_loss: 0.0000e+00 - val_accuracy: 0.4925 - 70s/epoch - 1s/step\n",
      "Epoch 17/30\n",
      "62/62 - 71s - loss: 0.0000e+00 - accuracy: 0.4847 - val_loss: 0.0000e+00 - val_accuracy: 0.4925 - 71s/epoch - 1s/step\n",
      "Epoch 18/30\n",
      "62/62 - 68s - loss: 0.0000e+00 - accuracy: 0.4858 - val_loss: 0.0000e+00 - val_accuracy: 0.4925 - 68s/epoch - 1s/step\n",
      "Epoch 19/30\n",
      "62/62 - 70s - loss: 0.0000e+00 - accuracy: 0.4868 - val_loss: 0.0000e+00 - val_accuracy: 0.4925 - 70s/epoch - 1s/step\n",
      "Epoch 20/30\n",
      "62/62 - 79s - loss: 0.0000e+00 - accuracy: 0.4888 - val_loss: 0.0000e+00 - val_accuracy: 0.4925 - 79s/epoch - 1s/step\n",
      "Epoch 21/30\n",
      "62/62 - 68s - loss: 0.0000e+00 - accuracy: 0.4827 - val_loss: 0.0000e+00 - val_accuracy: 0.4925 - 68s/epoch - 1s/step\n",
      "Epoch 22/30\n",
      "62/62 - 59s - loss: 0.0000e+00 - accuracy: 0.4863 - val_loss: 0.0000e+00 - val_accuracy: 0.4925 - 59s/epoch - 954ms/step\n",
      "Epoch 23/30\n",
      "62/62 - 65s - loss: 0.0000e+00 - accuracy: 0.4847 - val_loss: 0.0000e+00 - val_accuracy: 0.4925 - 65s/epoch - 1s/step\n",
      "Epoch 24/30\n",
      "62/62 - 71s - loss: 0.0000e+00 - accuracy: 0.4868 - val_loss: 0.0000e+00 - val_accuracy: 0.4925 - 71s/epoch - 1s/step\n",
      "Epoch 25/30\n",
      "62/62 - 69s - loss: 0.0000e+00 - accuracy: 0.4852 - val_loss: 0.0000e+00 - val_accuracy: 0.4925 - 69s/epoch - 1s/step\n",
      "Epoch 26/30\n",
      "62/62 - 78s - loss: 0.0000e+00 - accuracy: 0.4837 - val_loss: 0.0000e+00 - val_accuracy: 0.4925 - 78s/epoch - 1s/step\n",
      "Epoch 27/30\n",
      "62/62 - 62s - loss: 0.0000e+00 - accuracy: 0.4842 - val_loss: 0.0000e+00 - val_accuracy: 0.4925 - 62s/epoch - 994ms/step\n",
      "Epoch 28/30\n",
      "62/62 - 64s - loss: 0.0000e+00 - accuracy: 0.4858 - val_loss: 0.0000e+00 - val_accuracy: 0.4925 - 64s/epoch - 1s/step\n",
      "Epoch 29/30\n",
      "62/62 - 67s - loss: 0.0000e+00 - accuracy: 0.4852 - val_loss: 0.0000e+00 - val_accuracy: 0.4925 - 67s/epoch - 1s/step\n",
      "Epoch 30/30\n",
      "62/62 - 64s - loss: 0.0000e+00 - accuracy: 0.4842 - val_loss: 0.0000e+00 - val_accuracy: 0.4925 - 64s/epoch - 1s/step\n"
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "model_2 = train_model(model, \n",
    "                        train,\n",
    "                        valid, \n",
    "                        batch_size=32, \n",
    "                        epochs=30, \n",
    "                        steps_per_epoch=STEP_SIZE_TRAIN, \n",
    "                        validation_steps=STEP_SIZE_VALID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "32fea6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruba's CNN\n",
    "def create_model_3(input_shape):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(32, kernel_size=(2, 2), activation ='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(BatchNormalization()) # added\n",
    "    model.add(Conv2D(64, kernel_size = (2,2), activation ='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(BatchNormalization()) # added\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    optimizer = Adam(learning_rate=0.0001) # updated to lr=0.0001\n",
    "\n",
    "    model.compile(loss = \"binary_crossentropy\", \n",
    "              optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "510a0938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiating the model\n",
    "model_3 = create_model_3(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "3ae77bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "62/62 - 38s - loss: 0.7034 - accuracy: 0.6516 - val_loss: 0.7453 - val_accuracy: 0.4922 - 38s/epoch - 605ms/step\n",
      "Epoch 2/50\n",
      "62/62 - 41s - loss: 0.4514 - accuracy: 0.8006 - val_loss: 0.7386 - val_accuracy: 0.4922 - 41s/epoch - 657ms/step\n",
      "Epoch 3/50\n",
      "62/62 - 52s - loss: 0.3687 - accuracy: 0.8423 - val_loss: 0.7503 - val_accuracy: 0.5422 - 52s/epoch - 836ms/step\n",
      "Epoch 4/50\n",
      "62/62 - 29s - loss: 0.3161 - accuracy: 0.8739 - val_loss: 0.6689 - val_accuracy: 0.6203 - 29s/epoch - 471ms/step\n",
      "Epoch 5/50\n",
      "62/62 - 32s - loss: 0.2379 - accuracy: 0.9125 - val_loss: 0.9495 - val_accuracy: 0.5703 - 32s/epoch - 521ms/step\n",
      "Epoch 6/50\n",
      "62/62 - 29s - loss: 0.1998 - accuracy: 0.9329 - val_loss: 0.7145 - val_accuracy: 0.6516 - 29s/epoch - 471ms/step\n",
      "Epoch 7/50\n",
      "62/62 - 31s - loss: 0.1718 - accuracy: 0.9379 - val_loss: 0.5623 - val_accuracy: 0.7109 - 31s/epoch - 498ms/step\n",
      "Epoch 8/50\n",
      "62/62 - 33s - loss: 0.1744 - accuracy: 0.9349 - val_loss: 0.6352 - val_accuracy: 0.7328 - 33s/epoch - 527ms/step\n",
      "Epoch 9/50\n",
      "62/62 - 33s - loss: 0.1459 - accuracy: 0.9481 - val_loss: 0.4488 - val_accuracy: 0.7641 - 33s/epoch - 531ms/step\n",
      "Epoch 10/50\n",
      "62/62 - 28s - loss: 0.1362 - accuracy: 0.9542 - val_loss: 0.4433 - val_accuracy: 0.7953 - 28s/epoch - 451ms/step\n",
      "Epoch 11/50\n",
      "62/62 - 28s - loss: 0.1325 - accuracy: 0.9502 - val_loss: 0.3861 - val_accuracy: 0.8328 - 28s/epoch - 449ms/step\n",
      "Epoch 12/50\n",
      "62/62 - 27s - loss: 0.1132 - accuracy: 0.9639 - val_loss: 0.3491 - val_accuracy: 0.8547 - 27s/epoch - 442ms/step\n",
      "Epoch 13/50\n",
      "62/62 - 29s - loss: 0.1097 - accuracy: 0.9593 - val_loss: 0.3523 - val_accuracy: 0.8516 - 29s/epoch - 472ms/step\n",
      "Epoch 14/50\n",
      "62/62 - 28s - loss: 0.0879 - accuracy: 0.9741 - val_loss: 0.3552 - val_accuracy: 0.8687 - 28s/epoch - 459ms/step\n",
      "Epoch 15/50\n",
      "62/62 - 32s - loss: 0.0858 - accuracy: 0.9730 - val_loss: 0.3958 - val_accuracy: 0.8625 - 32s/epoch - 513ms/step\n",
      "Epoch 16/50\n",
      "62/62 - 30s - loss: 0.1129 - accuracy: 0.9557 - val_loss: 0.3828 - val_accuracy: 0.8516 - 30s/epoch - 489ms/step\n",
      "Epoch 17/50\n",
      "62/62 - 31s - loss: 0.0841 - accuracy: 0.9725 - val_loss: 0.3816 - val_accuracy: 0.8484 - 31s/epoch - 503ms/step\n",
      "Epoch 18/50\n",
      "62/62 - 30s - loss: 0.0842 - accuracy: 0.9685 - val_loss: 0.4448 - val_accuracy: 0.8375 - 30s/epoch - 483ms/step\n",
      "Epoch 19/50\n",
      "62/62 - 31s - loss: 0.0719 - accuracy: 0.9807 - val_loss: 0.3580 - val_accuracy: 0.8687 - 31s/epoch - 493ms/step\n",
      "Epoch 20/50\n",
      "62/62 - 31s - loss: 0.0758 - accuracy: 0.9756 - val_loss: 0.3507 - val_accuracy: 0.8719 - 31s/epoch - 492ms/step\n",
      "Epoch 21/50\n",
      "62/62 - 30s - loss: 0.0771 - accuracy: 0.9746 - val_loss: 0.3456 - val_accuracy: 0.8906 - 30s/epoch - 482ms/step\n",
      "Epoch 22/50\n",
      "62/62 - 29s - loss: 0.0651 - accuracy: 0.9761 - val_loss: 0.3299 - val_accuracy: 0.8719 - 29s/epoch - 465ms/step\n",
      "Epoch 23/50\n",
      "62/62 - 31s - loss: 0.0741 - accuracy: 0.9715 - val_loss: 0.3786 - val_accuracy: 0.8453 - 31s/epoch - 494ms/step\n",
      "Epoch 24/50\n",
      "62/62 - 26s - loss: 0.0645 - accuracy: 0.9817 - val_loss: 0.4264 - val_accuracy: 0.8484 - 26s/epoch - 414ms/step\n",
      "Epoch 25/50\n",
      "62/62 - 27s - loss: 0.0579 - accuracy: 0.9822 - val_loss: 0.3630 - val_accuracy: 0.8656 - 27s/epoch - 431ms/step\n",
      "Epoch 26/50\n",
      "62/62 - 29s - loss: 0.0659 - accuracy: 0.9771 - val_loss: 0.4537 - val_accuracy: 0.8422 - 29s/epoch - 467ms/step\n",
      "Epoch 27/50\n",
      "62/62 - 46s - loss: 0.0732 - accuracy: 0.9786 - val_loss: 0.3965 - val_accuracy: 0.8625 - 46s/epoch - 745ms/step\n",
      "Epoch 28/50\n",
      "62/62 - 28s - loss: 0.0616 - accuracy: 0.9812 - val_loss: 0.3793 - val_accuracy: 0.8500 - 28s/epoch - 455ms/step\n",
      "Epoch 29/50\n",
      "62/62 - 28s - loss: 0.0618 - accuracy: 0.9812 - val_loss: 0.4117 - val_accuracy: 0.8562 - 28s/epoch - 458ms/step\n",
      "Epoch 30/50\n",
      "62/62 - 34s - loss: 0.0441 - accuracy: 0.9868 - val_loss: 0.3988 - val_accuracy: 0.8391 - 34s/epoch - 543ms/step\n",
      "Epoch 31/50\n",
      "62/62 - 25s - loss: 0.0551 - accuracy: 0.9802 - val_loss: 0.3857 - val_accuracy: 0.8484 - 25s/epoch - 405ms/step\n",
      "Epoch 32/50\n",
      "62/62 - 23s - loss: 0.0576 - accuracy: 0.9837 - val_loss: 0.3689 - val_accuracy: 0.8672 - 23s/epoch - 378ms/step\n",
      "Epoch 33/50\n",
      "62/62 - 24s - loss: 0.0474 - accuracy: 0.9873 - val_loss: 0.4237 - val_accuracy: 0.8547 - 24s/epoch - 389ms/step\n",
      "Epoch 34/50\n",
      "62/62 - 22s - loss: 0.0390 - accuracy: 0.9908 - val_loss: 0.4231 - val_accuracy: 0.8656 - 22s/epoch - 362ms/step\n",
      "Epoch 35/50\n",
      "62/62 - 22s - loss: 0.0654 - accuracy: 0.9791 - val_loss: 0.3883 - val_accuracy: 0.8625 - 22s/epoch - 362ms/step\n",
      "Epoch 36/50\n",
      "62/62 - 23s - loss: 0.0546 - accuracy: 0.9807 - val_loss: 0.4324 - val_accuracy: 0.8687 - 23s/epoch - 367ms/step\n",
      "Epoch 37/50\n",
      "62/62 - 22s - loss: 0.0375 - accuracy: 0.9888 - val_loss: 0.4502 - val_accuracy: 0.8344 - 22s/epoch - 360ms/step\n",
      "Epoch 38/50\n",
      "62/62 - 23s - loss: 0.0370 - accuracy: 0.9884 - val_loss: 0.4336 - val_accuracy: 0.8516 - 23s/epoch - 364ms/step\n",
      "Epoch 39/50\n",
      "62/62 - 22s - loss: 0.0455 - accuracy: 0.9842 - val_loss: 0.4345 - val_accuracy: 0.8578 - 22s/epoch - 358ms/step\n",
      "Epoch 40/50\n",
      "62/62 - 22s - loss: 0.0426 - accuracy: 0.9852 - val_loss: 0.4179 - val_accuracy: 0.8438 - 22s/epoch - 360ms/step\n",
      "Epoch 41/50\n",
      "62/62 - 23s - loss: 0.0407 - accuracy: 0.9873 - val_loss: 0.3820 - val_accuracy: 0.8781 - 23s/epoch - 370ms/step\n",
      "Epoch 42/50\n",
      "62/62 - 22s - loss: 0.0383 - accuracy: 0.9863 - val_loss: 0.3942 - val_accuracy: 0.8734 - 22s/epoch - 361ms/step\n",
      "Epoch 43/50\n",
      "62/62 - 22s - loss: 0.0436 - accuracy: 0.9878 - val_loss: 0.3833 - val_accuracy: 0.8578 - 22s/epoch - 359ms/step\n",
      "Epoch 44/50\n",
      "62/62 - 23s - loss: 0.0347 - accuracy: 0.9893 - val_loss: 0.3975 - val_accuracy: 0.8609 - 23s/epoch - 371ms/step\n",
      "Epoch 45/50\n",
      "62/62 - 23s - loss: 0.0454 - accuracy: 0.9868 - val_loss: 0.4254 - val_accuracy: 0.8656 - 23s/epoch - 365ms/step\n",
      "Epoch 46/50\n",
      "62/62 - 25s - loss: 0.0284 - accuracy: 0.9899 - val_loss: 0.4396 - val_accuracy: 0.8578 - 25s/epoch - 399ms/step\n",
      "Epoch 47/50\n",
      "62/62 - 25s - loss: 0.0261 - accuracy: 0.9934 - val_loss: 0.4188 - val_accuracy: 0.8484 - 25s/epoch - 403ms/step\n",
      "Epoch 48/50\n",
      "62/62 - 26s - loss: 0.0267 - accuracy: 0.9924 - val_loss: 0.4083 - val_accuracy: 0.8594 - 26s/epoch - 413ms/step\n",
      "Epoch 49/50\n",
      "62/62 - 26s - loss: 0.0293 - accuracy: 0.9929 - val_loss: 0.4086 - val_accuracy: 0.8703 - 26s/epoch - 412ms/step\n",
      "Epoch 50/50\n",
      "62/62 - 24s - loss: 0.0306 - accuracy: 0.9908 - val_loss: 0.4393 - val_accuracy: 0.8594 - 24s/epoch - 385ms/step\n"
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "model_3 = train_model(model_3, \n",
    "                        train,\n",
    "                        valid, \n",
    "                        batch_size=32, \n",
    "                        epochs=50, \n",
    "                        steps_per_epoch=STEP_SIZE_TRAIN, \n",
    "                        validation_steps=STEP_SIZE_VALID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "e328b139",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_4(input_shape):\n",
    "    model1 = Sequential()\n",
    "    model1.add(Conv2D(32, (3, 3),\n",
    "                     input_shape=input_shape))\n",
    "    model1.add(Activation('relu'))\n",
    "    model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model1.add(Dropout(0.25))\n",
    "    model1.add(Conv2D(64, (3, 3)))\n",
    "#     model1.add(Activation('relu'))\n",
    "#     model1.add(Conv2D(128, (3, 3)))\n",
    "    model1.add(Activation('relu'))\n",
    "    model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(BatchNormalization()) # added\n",
    "    model1.add(Dropout(0.25))\n",
    "\n",
    "    model1.add(Flatten())\n",
    "\n",
    "    model1.add(Dense(1, activation='softmax'))\n",
    "\n",
    "    optimizer = Adam(learning_rate=0.0001) # updated to lr=0.0001\n",
    "    model1.compile(optimizer=optimizer, \n",
    "                  loss=\"binary_crossentropy\", \n",
    "              metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "92f053a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "62/62 - 64s - loss: 0.0000e+00 - accuracy: 0.4817 - val_loss: 0.0000e+00 - val_accuracy: 0.4922 - 64s/epoch - 1s/step\n",
      "Epoch 2/30\n",
      "62/62 - 60s - loss: 0.0000e+00 - accuracy: 0.4842 - val_loss: 0.0000e+00 - val_accuracy: 0.4969 - 60s/epoch - 967ms/step\n",
      "Epoch 3/30\n",
      "62/62 - 63s - loss: 0.0000e+00 - accuracy: 0.4837 - val_loss: 0.0000e+00 - val_accuracy: 0.4922 - 63s/epoch - 1s/step\n",
      "Epoch 4/30\n",
      "62/62 - 62s - loss: 0.0000e+00 - accuracy: 0.4863 - val_loss: 0.0000e+00 - val_accuracy: 0.4906 - 62s/epoch - 1s/step\n",
      "Epoch 5/30\n",
      "62/62 - 59s - loss: 0.0000e+00 - accuracy: 0.4847 - val_loss: 0.0000e+00 - val_accuracy: 0.4984 - 59s/epoch - 957ms/step\n",
      "Epoch 6/30\n",
      "62/62 - 60s - loss: 0.0000e+00 - accuracy: 0.4858 - val_loss: 0.0000e+00 - val_accuracy: 0.4953 - 60s/epoch - 974ms/step\n",
      "Epoch 7/30\n",
      "62/62 - 62s - loss: 0.0000e+00 - accuracy: 0.4852 - val_loss: 0.0000e+00 - val_accuracy: 0.4891 - 62s/epoch - 995ms/step\n",
      "Epoch 8/30\n",
      "62/62 - 61s - loss: 0.0000e+00 - accuracy: 0.4852 - val_loss: 0.0000e+00 - val_accuracy: 0.4891 - 61s/epoch - 987ms/step\n",
      "Epoch 9/30\n",
      "62/62 - 60s - loss: 0.0000e+00 - accuracy: 0.4832 - val_loss: 0.0000e+00 - val_accuracy: 0.4969 - 60s/epoch - 966ms/step\n",
      "Epoch 10/30\n",
      "62/62 - 60s - loss: 0.0000e+00 - accuracy: 0.4842 - val_loss: 0.0000e+00 - val_accuracy: 0.4938 - 60s/epoch - 973ms/step\n",
      "Epoch 11/30\n",
      "62/62 - 60s - loss: 0.0000e+00 - accuracy: 0.4847 - val_loss: 0.0000e+00 - val_accuracy: 0.4906 - 60s/epoch - 971ms/step\n",
      "Epoch 12/30\n",
      "62/62 - 64s - loss: 0.0000e+00 - accuracy: 0.4842 - val_loss: 0.0000e+00 - val_accuracy: 0.5000 - 64s/epoch - 1s/step\n",
      "Epoch 13/30\n",
      "62/62 - 65s - loss: 0.0000e+00 - accuracy: 0.4863 - val_loss: 0.0000e+00 - val_accuracy: 0.4906 - 65s/epoch - 1s/step\n",
      "Epoch 14/30\n",
      "62/62 - 64s - loss: 0.0000e+00 - accuracy: 0.4837 - val_loss: 0.0000e+00 - val_accuracy: 0.4953 - 64s/epoch - 1s/step\n",
      "Epoch 15/30\n",
      "62/62 - 62s - loss: 0.0000e+00 - accuracy: 0.4842 - val_loss: 0.0000e+00 - val_accuracy: 0.4859 - 62s/epoch - 1s/step\n",
      "Epoch 16/30\n",
      "62/62 - 64s - loss: 0.0000e+00 - accuracy: 0.4837 - val_loss: 0.0000e+00 - val_accuracy: 0.4969 - 64s/epoch - 1s/step\n",
      "Epoch 17/30\n",
      "62/62 - 63s - loss: 0.0000e+00 - accuracy: 0.4868 - val_loss: 0.0000e+00 - val_accuracy: 0.4922 - 63s/epoch - 1s/step\n",
      "Epoch 18/30\n",
      "62/62 - 66s - loss: 0.0000e+00 - accuracy: 0.4837 - val_loss: 0.0000e+00 - val_accuracy: 0.4875 - 66s/epoch - 1s/step\n",
      "Epoch 19/30\n",
      "62/62 - 61s - loss: 0.0000e+00 - accuracy: 0.4863 - val_loss: 0.0000e+00 - val_accuracy: 0.4953 - 61s/epoch - 978ms/step\n",
      "Epoch 20/30\n",
      "62/62 - 71s - loss: 0.0000e+00 - accuracy: 0.4863 - val_loss: 0.0000e+00 - val_accuracy: 0.4906 - 71s/epoch - 1s/step\n",
      "Epoch 21/30\n",
      "62/62 - 63s - loss: 0.0000e+00 - accuracy: 0.4863 - val_loss: 0.0000e+00 - val_accuracy: 0.4938 - 63s/epoch - 1s/step\n",
      "Epoch 22/30\n",
      "62/62 - 69s - loss: 0.0000e+00 - accuracy: 0.4832 - val_loss: 0.0000e+00 - val_accuracy: 0.4922 - 69s/epoch - 1s/step\n",
      "Epoch 23/30\n",
      "62/62 - 76s - loss: 0.0000e+00 - accuracy: 0.4863 - val_loss: 0.0000e+00 - val_accuracy: 0.4875 - 76s/epoch - 1s/step\n",
      "Epoch 24/30\n",
      "62/62 - 69s - loss: 0.0000e+00 - accuracy: 0.4852 - val_loss: 0.0000e+00 - val_accuracy: 0.4953 - 69s/epoch - 1s/step\n",
      "Epoch 25/30\n",
      "62/62 - 67s - loss: 0.0000e+00 - accuracy: 0.4822 - val_loss: 0.0000e+00 - val_accuracy: 0.4938 - 67s/epoch - 1s/step\n",
      "Epoch 26/30\n",
      "62/62 - 67s - loss: 0.0000e+00 - accuracy: 0.4878 - val_loss: 0.0000e+00 - val_accuracy: 0.5000 - 67s/epoch - 1s/step\n",
      "Epoch 27/30\n",
      "62/62 - 78s - loss: 0.0000e+00 - accuracy: 0.4873 - val_loss: 0.0000e+00 - val_accuracy: 0.4953 - 78s/epoch - 1s/step\n",
      "Epoch 28/30\n",
      "62/62 - 69s - loss: 0.0000e+00 - accuracy: 0.4832 - val_loss: 0.0000e+00 - val_accuracy: 0.4922 - 69s/epoch - 1s/step\n",
      "Epoch 29/30\n",
      "62/62 - 61s - loss: 0.0000e+00 - accuracy: 0.4832 - val_loss: 0.0000e+00 - val_accuracy: 0.4906 - 61s/epoch - 982ms/step\n",
      "Epoch 30/30\n",
      "62/62 - 64s - loss: 0.0000e+00 - accuracy: 0.4858 - val_loss: 0.0000e+00 - val_accuracy: 0.4953 - 64s/epoch - 1s/step\n"
     ]
    }
   ],
   "source": [
    "# Initiating the model\n",
    "model_4 = create_model_4(input_shape)\n",
    "\n",
    "# training the model\n",
    "model_4 = train_model(model_4, \n",
    "                        train,\n",
    "                        valid, \n",
    "                        batch_size=32, \n",
    "                        epochs=30, \n",
    "                        steps_per_epoch=STEP_SIZE_TRAIN, \n",
    "                        validation_steps=STEP_SIZE_VALID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "75f5aa90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_5(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', input_shape=input_shape))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Conv2D(32, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    model.add(BatchNormalization()) # added\n",
    "    \n",
    "#     model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "#     model.add(Activation('relu'))\n",
    "    \n",
    "    model.add(Conv2D(64, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(Dropout(0.25)) # regularization\n",
    "    model.add(BatchNormalization()) # added\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizer='Adamax', \n",
    "                  loss=\"binary_crossentropy\", \n",
    "              metrics=[\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee623c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    }
   ],
   "source": [
    "# Initiating the model\n",
    "model_5 = create_model_5(input_shape)\n",
    "\n",
    "# training the model\n",
    "model_5 = train_model(model_5, \n",
    "                        train,\n",
    "                        valid, \n",
    "                        batch_size=32, \n",
    "                        epochs=30, \n",
    "                        steps_per_epoch=STEP_SIZE_TRAIN, \n",
    "                        validation_steps=STEP_SIZE_VALID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fafc35c",
   "metadata": {},
   "source": [
    "## 3 Choosing the best CNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3945b3",
   "metadata": {},
   "source": [
    "- Model 1 - ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e57c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "t = f.suptitle('Basic ResNet50 CNN Performance', fontsize=12)\n",
    "f.subplots_adjust(top=0.85, wspace=0.3)\n",
    "\n",
    "epoch_list = list(range(0, 10))\n",
    "ax1.plot(epoch_list, ResNet50_model.history['accuracy'], label='Train Accuracy')\n",
    "ax1.plot(epoch_list, ResNet50_model.history['val_accuracy'], label='Validation Accuracy')\n",
    "\n",
    "ax1.set_xticks(np.arange(0, 10, 1))\n",
    "ax1.set_ylabel('Accuracy Value')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_title('Accuracy')\n",
    "l1 = ax1.legend(loc=\"best\")\n",
    "\n",
    "ax2.plot(epoch_list, ResNet50_model.history['loss'], label='Train Loss')\n",
    "ax2.plot(epoch_list, ResNet50_model.history['val_loss'], label='Validation Loss')\n",
    "ax2.set_xticks(np.arange(0, 10, 1))\n",
    "ax2.set_ylabel('Loss Value')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_title('Loss')\n",
    "l2 = ax2.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfd3a11",
   "metadata": {},
   "source": [
    "- Model 2 CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf96edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "t = f.suptitle('Basic CNN Performance', fontsize=12)\n",
    "f.subplots_adjust(top=0.85, wspace=0.3)\n",
    "\n",
    "epoch_list = list(range(0, 10))\n",
    "ax1.plot(epoch_list, model.history['accuracy'], label='Train Accuracy')\n",
    "ax1.plot(epoch_list, model.history['val_accuracy'], label='Validation Accuracy')\n",
    "\n",
    "ax1.set_xticks(np.arange(0, 10, 1))\n",
    "ax1.set_ylabel('Accuracy Value')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_title('Accuracy')\n",
    "l1 = ax1.legend(loc=\"best\")\n",
    "\n",
    "ax2.plot(epoch_list, model.history['loss'], label='Train Loss')\n",
    "ax2.plot(epoch_list, model.history['val_loss'], label='Validation Loss')\n",
    "ax2.set_xticks(np.arange(0, 10, 1))\n",
    "ax2.set_ylabel('Loss Value')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_title('Loss')\n",
    "l2 = ax2.legend(loc=\"best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb4a539",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739ec6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(r.history['accuracy'], label='acc', color='red')\n",
    "plt.plot(r.history['val_accuracy'], label='val_acc', color='darkblue')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
